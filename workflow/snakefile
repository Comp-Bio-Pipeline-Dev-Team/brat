from os.path import join as pj
from os.path import basename
from os import environ
import pandas as pd
import re
import glob
from snake_utils.snake_functions import comb_filepaths, make_fp_dict, picard_calculate_strandedness


RAW_SEQ_IN =  config['raw_seq_in']
METADATA = pd.read_csv(config['metadata_file'], sep='\t')
SAMPLE_LIST = METADATA['sampleid'].tolist()
MINI_SAMPLE_LIST = 'NA241plus_AUT_S7_L004'

## user input additional parameters for cutadapt, star, and rsem (optional)
EXTRA_CUTADAPT_PARAMS = config['cutadapt_params']
EXTRA_STAR_PARAMS = config['star_params']
EXTRA_RSEM_PARAMS = config['rsem_params']

## user input genome index files and the name of the index 
ALIGN_FASTA = config['align_to_fasta']
ALIGN_ANNOT = config['align_to_gtf']
ALIGN_INDEX_NAME = config['align_to_name']
PICARD_REFFLAT = config['picard_refFlat']
PICARD_RRNA_INTERVAL = config['picard_rrna_interval_list']

## to move wanted genome index files for star, rsem, and picard into the pipeline's working directory so bind points dont break 
REF_DIR = "user_reference_files"
NEW_FASTA_PATH= pj(REF_DIR, basename(ALIGN_FASTA))
NEW_ANNOT_PATH = pj(REF_DIR, basename(ALIGN_ANNOT))
NEW_PICARD_REFFLAT = pj(REF_DIR, basename(PICARD_REFFLAT))
NEW_PICARD_RRNA_INTERVAL = pj(REF_DIR, basename(PICARD_RRNA_INTERVAL))

READ_LENGTH = config['raw_seq_read_length']

## for rsem index generation 
RSEM_INDEX_DIR = pj("reference_indices/rsem", ALIGN_INDEX_NAME)


sample_fp_dict = make_fp_dict(metadata_df=METADATA,
                              dataset_dir=RAW_SEQ_IN)


## conda environment
FASTQC_CONDA = "envs/fastqc_env.yml"
MULTIQC_CONDA = "envs/multiqc_env.yml"

## singularities (preferred) - NEED TO DEBUG THIS ON ALPINE
FASTQC_SING = "docker://madiapgar/bulk_rna_seq:fastqc-v0.12.1"
MULTIQC_SING = "docker://madiapgar/bulk_rna_seq:multiqc-v1.26"
CUTADAPT_SING = "docker://madiapgar/bulk_rna_seq:cutadapt-v4.2"
STAR_SING = "docker://madiapgar/bulk_rna_seq:star-v2.7.10b"
RSEM_SING = "docker://madiapgar/bulk_rna_seq:rsem-v1.3.3"
PICARD_SING = "docker://madiapgar/bulk_rna_seq:picard-v2.27.5"

## functions? to use these as inputs you need to have the function defined in the snakefile which is dumb but whatever
## added wildcard constraints so my input function would stop breaking, this regex works for the current sampleids but will probs have to
## alter it in the future
## regex to only include sample id but not full path: [^/]* (excludes anything with forward slashes in it) - ADD EXCLUDING WHITE SPACE (\s)
## current regex only selects letters, numbers, and underscores: \\w+
wildcard_constraints:
    sample="\\w+"

def pull_rawSeq_fps(wildcards):
    raw_files = sample_fp_dict[wildcards.sample]
    return(raw_files)


# Set apptainer bindings NOT A PERMANENT SOLUTION
## idk if this is going to work
##os.environ["APPTAINER_BIND"] = "/scratch/alpine/mapgar@xsede.org:/scratch/alpine/mapgar@xsede.org"

rule all:
    input:
        expand("bulk_RNAseq_out/pretrimming_fastqc/{sample}/",
               sample=SAMPLE_LIST),
        "bulk_RNAseq_out/pretrimming_multiqc_report.html",
        expand("bulk_RNAseq_out/cutadapt/{sample}/{sample}_R1_trimmed.fastq.gz",
               sample=SAMPLE_LIST),
        expand("bulk_RNAseq_out/cutadapt/{sample}/{sample}_R2_trimmed.fastq.gz",
               sample=SAMPLE_LIST),
        expand("bulk_RNAseq_out/cutadapt/{sample}/{sample}_cutadapt.log",
               sample=SAMPLE_LIST),
        expand("bulk_RNAseq_out/cutadapt/{sample}/{sample}_cutadapt.err",
               sample=SAMPLE_LIST),
        expand("bulk_RNAseq_out/posttrimming_fastqc/{sample}/",
                sample=SAMPLE_LIST),
        "bulk_RNAseq_out/posttrimming_multiqc_report.html",
        NEW_FASTA_PATH,
        NEW_ANNOT_PATH,
        NEW_PICARD_RRNA_INTERVAL,
        NEW_PICARD_REFFLAT,
        "reference_indices/rsem/",
        "reference_indices/star/",
        expand("bulk_RNAseq_out/star_alignment/{sample}/",
               sample=SAMPLE_LIST),
        expand("bulk_RNAseq_out/picard/{sample}/{sample}.picard.metrics.txt",
               sample=SAMPLE_LIST),
        expand("bulk_RNAseq_out/picard/{sample}/{sample}.picard.insertSize.txt",
               sample=SAMPLE_LIST),
        expand("bulk_RNAseq_out/picard/{sample}/{sample}.picard.insertSize_histogram.pdf",
               sample=SAMPLE_LIST),
        "bulk_RNAseq_out/picard/allSample_picard_metrics.csv",
        expand("bulk_RNAseq_out/rsem_quantification/{sample}/",
               sample=SAMPLE_LIST)
        

##### SUBWORKFLOW ONE!! #####
## set up to run for all paired samples - in .csv file that tells you what pairs on, read in as a dict of the file names per sample
## output directory per samples
## can keep threads=2 and ntasks=1
##### madis notes: #####
## both examples under "input:" work to pull both forward/reverse reads in for each sample 
## could also do this: inFiles = expand(pj(RAW_SEQ_IN, "{{sample}}_{read}.fastq.gz"), read=READS) 
rule run_pretrimming_fastqc:
    input:
        inFiles = pull_rawSeq_fps
    output:
        outDir = directory("bulk_RNAseq_out/pretrimming_fastqc/{sample}/")
    singularity:
        FASTQC_SING
    conda:
        FASTQC_CONDA
    params:
        threads = 2
    shell:
        """
        echo "running fastqc"

        mkdir {output.outDir}

        fastqc {input.inFiles} -o {output.outDir} --threads {params.threads}
        """



rule run_pretrimming_multiqc:
    input:
        ## tells snakemake to wait for all fastqc outputs to be made before starting this rule since all of them are required for this rule
        inDirs = expand("bulk_RNAseq_out/pretrimming_fastqc/{sample}/",
                         sample=SAMPLE_LIST)
    output:
        outFile = "bulk_RNAseq_out/pretrimming_multiqc_report.html"
    singularity:
        MULTIQC_SING
    conda:
        MULTIQC_CONDA
    params:
        outDir = "bulk_RNAseq_out/",
        multiqcFilename = "pretrimming_multiqc_report.html"
    shell:
        """
        echo "running multiqc"

        multiqc {input.inDirs} -o {params.outDir} --filename {params.multiqcFilename} .
        """

## {params.inDir}*/


## had to add set +o pipefail; to beginning of totalScores command so that snakemake doesn't detect non-zero error codes and fail
## (snakemake was detecing 141 error codes for first two steps in pipe bc pipe is so long they complete before later steps do)
## added '<' to zcat command so its portable to macos, macos prefers gzcat or zcat < but not plain zcat (its dumb)
## can use to check the error codes of all components of a pipe: echo ${PIPESTATUS[@]}
## snakemake can insert {params.whatever} inside the double quotes of chemistryFlag variable and its evaluated as the value (30)
## add {params.user_added_cutadaptParams} to the command!
rule run_cutadapt:
    input:
        inFiles = pull_rawSeq_fps
    output:
        forwardTrimmed = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_R1_trimmed.fastq.gz",
        reverseTrimmed = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_R2_trimmed.fastq.gz"
    singularity:
        CUTADAPT_SING
    ##conda:
    log:
        log = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_cutadapt.log",
        error = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_cutadapt.err"
    params:
        adapter_threePrime = "AGATCGGAAGAG",
        qualTrim = 30,
        minReadLen = 10,
        n_threads = 4, ## n_threads = cpus_per_task*2
        user_added_cutadaptParams = EXTRA_CUTADAPT_PARAMS
    shell:
        """
        totalScores=$( set +o pipefail; zcat < {input.inFiles[0]} | awk 'NR%4==0' | sed 100q | grep -o . | sort | uniq | wc -l )

        if [ ${{totalScores}} -eq 4 ];
        then
            # NovaSeq now bins qc values to 2, 12, 23, 37 (if using NovaSeq), also use --nextseq-trim= instead of -q since NovaSeq is 2 color chemistry
            echo "Quality scores are binned to 4 values (your score = ${{totalScores}}) therefore, using 2-color chemistry; running --nextseq-trim instead of -q"
            chemistryFlag="--nextseq-trim={params.qualTrim}"
        else
            echo "Quality scores are not binned (your score = ${{totalScores}}), therefore use -q"
            chemistryFlag="-q {params.qualTrim}"
        fi

        cutadapt -a {params.adapter_threePrime} \
                 -A {params.adapter_threePrime} \
                 "${{chemistryFlag}}" \
                 --minimum-length={params.minReadLen} \
                 -j {params.n_threads} \
                 --pair-filter=any \
                 -o {output.forwardTrimmed} \
                 -p {output.reverseTrimmed} {params.user_added_cutadaptParams} \
                 {input.inFiles[0]} {input.inFiles[1]}>{log.log} \
                 2>{log.error} 
        """

## can reuse previous rules this way!
## can alter the input, output, params, etc directives, anything not defined here will be inherited from the original rule
## CANNOT alter the execution step (i.e. shell)
## running fastqc on samples posttrimming via cutadapt
use rule run_pretrimming_fastqc as run_posttrimming_fastqc with:
    input:
        inFiles = ["bulk_RNAseq_out/cutadapt/{sample}/{sample}_R1_trimmed.fastq.gz", "bulk_RNAseq_out/cutadapt/{sample}/{sample}_R2_trimmed.fastq.gz"]
    output:
        outDir = directory("bulk_RNAseq_out/posttrimming_fastqc/{sample}/")


## running multiqc on samples posttrimming via cutadapt 
## if you include a directive to overwrite here (i.e. params), you'll need to specify all arguments underneath it,
## even if they don't change
use rule run_pretrimming_multiqc as run_posttrimming_multiqc with:
    input:
        inDirs = expand("bulk_RNAseq_out/posttrimming_fastqc/{sample}/",
                        sample=SAMPLE_LIST)
    output:
        outFile = "bulk_RNAseq_out/posttrimming_multiqc_report.html"
    params:
        outDir = "bulk_RNAseq_out/",
        multiqcFilename = "posttrimming_multiqc_report.html"



## move input fasta/gtf files into the test_snake_w_slurm (or whatever working) directory so bind points dont break
rule move_reference_files:
    input:
        fastaFile = ALIGN_FASTA,
        annotFile = ALIGN_ANNOT,
        refFlat = PICARD_REFFLAT,
        riboIntList = PICARD_RRNA_INTERVAL
    output:
        new_loc_fasta = NEW_FASTA_PATH,
        new_loc_annot = NEW_ANNOT_PATH,
        new_refFlat = NEW_PICARD_REFFLAT,
        new_riboIntList = NEW_PICARD_RRNA_INTERVAL
    params:
        ref_directory = REF_DIR
    shell:
        """
        mkdir -p {params.ref_directory}

        ## rsem and star reference files
        mv {input.fastaFile} {output.new_loc_fasta}
        echo "moved genome fasta file to working directory!"

        mv {input.annotFile} {output.new_loc_annot}
        echo "moved genome annotation file to working directory!"

        ## picard reference files
        mv {input.refFlat} {output.new_refFlat}
        echo "moved picard refFlat file to working directory!"

        mv {input.riboIntList} {output.new_riboIntList}
        echo "moved picard rRNA interval list to working directory!"
        """



## wont want to run this rule unless its needed - config option/looking at star log files for %uniquely_mapped v %percent_multimapped v %unmapped thresholds for when its run
## rsem genome index generation (probs will be moved to a subworkflow but its here for now) 
## finally works using outside fasta/gtf files with above rule (yay!!)
rule generate_rsem_index:
    input:
        fastaFile = NEW_FASTA_PATH,
        annotFile = NEW_ANNOT_PATH
    output:
        rsem_index_dir = directory("reference_indices/rsem/")
    singularity:
        RSEM_SING
    params:
        rsem_index_name = ALIGN_INDEX_NAME
    shell:
        """
        mkdir -p {output.rsem_index_dir}

        rsem-prepare-reference --gtf {input.annotFile} {input.fastaFile} {output.rsem_index_dir}/{params.rsem_index_name}
        """

## got errors about bc not being installed in container (it wasnt so thats fine) and out of memory error which was weird 
## increased memory to check if it does okay - needed 50GBs of memory 
## check if read length set to zero or do want to infer? can take user input parameter (greater than 0) - i cant remember why we were doing this
rule generate_star_index:
    input:
        pretrimming_multiqc_report = "bulk_RNAseq_out/pretrimming_multiqc_report_data/multiqc_general_stats.txt",
        fastaFile = NEW_FASTA_PATH,
        annotFile = NEW_ANNOT_PATH
    output:
        star_index_dir = directory("reference_indices/star/")
    singularity:
        STAR_SING
    params:
        input_read_length = READ_LENGTH,
        n_threads = 22, ## n_threads = cpus_per_task*2
        feature_type = "exon"
    shell:
        """
        if [ {params.input_read_length} -eq 0 ];
        then
            ## count read lengths in raw seq files for STAR
            readLength=$( awk '{{print $5}}' {input.pretrimming_multiqc_report} | tail -n+2 | head -n 1 )
            overheadLength=$( echo "$((${{readLength}}-1))" )
            echo "Inferred sequence read length is: ${{overheadLength}}"
        else
            overheadLength={params.input_read_length}
            echo "User input sequence read length is: ${{overheadLength}}"
        fi

        ## calculation of genomeLength - remove headers, remove whitespace and new lines, then count number of characters (bases)
        genomeLength=$(cat {input.fastaFile} | grep -v "^>" | tr -d [:space:] | wc -m)
        echo "The length of the provided genome is: ${{genomeLength}} bases"

        ## calculation of genomeSAindex: min(14, log2(GenomeLength)/2 - 1)
        genome_indexSize=$(printf %.0f $(echo "((l(${{genomeLength}})/l(2))/2)-1" | bc -l))
        if [[ ${{genome_indexSize}} -ge 14 ]]; then
            genome_indexSize=14
        fi
        echo "Using genome index size of: ${{genome_indexSize}}"

        ## making output directory
        mkdir -p {output.star_index_dir}

        STAR --runThreadN {params.n_threads} \
             --runMode genomeGenerate \
             --genomeDir {output.star_index_dir} \
             --genomeFastaFiles {input.fastaFile} \
             --sjdbGTFfile {input.annotFile} \
             --sjdbGTFfeatureExon {params.feature_type} \
             --sjdbOverhang ${{overheadLength}} \
             --genomeSAindexNbases ${{genome_indexSize}}

        """

## actual rule for star!!
rule run_star_alignment:
    input:
        pretrimming_multiqc_report = "bulk_RNAseq_out/pretrimming_multiqc_report_data/multiqc_general_stats.txt",
        star_genome_index_dir = "reference_indices/star/",
        forwardTrimmed = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_R1_trimmed.fastq.gz",
        reverseTrimmed = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_R2_trimmed.fastq.gz",
        annotFile = NEW_ANNOT_PATH
    output:
        star_out_dir = directory("bulk_RNAseq_out/star_alignment/{sample}/"),
        star_out_coordFile = "bulk_RNAseq_out/star_alignment/{sample}/{sample}.Aligned.sortedByCoord.out.bam",
        star_out_transcriptFile = "bulk_RNAseq_out/star_alignment/{sample}/{sample}.Aligned.toTranscriptome.out.bam"
    singularity:
        STAR_SING
    params:
        input_read_length = READ_LENGTH,
        n_threads = 18, ## n_threads = cpus_per_task*1.5, madi note: doubling the nthreads per cpu caused out of memory errors within the first minute of the job running
        star_sample_prefix = "bulk_RNAseq_out/star_alignment/{sample}/{sample}.", ## may need another decoy here bc snakemake takes the front slash off of the output fp above (now the file names look funky)
        user_added_starParams = EXTRA_STAR_PARAMS
    shell:
        """
        ## have to redo read length calculation, maybe not the best? but idk what else to do 
        if [ {params.input_read_length} -eq 0 ];
        then
            ## count read lengths in raw seq files for STAR
            readLength=$( awk '{{print $5}}' {input.pretrimming_multiqc_report} | tail -n+2 | head -n 1 )
            overheadLength=$( echo "$((${{readLength}}-1))" )
            echo "Inferred sequence read length is: ${{overheadLength}}"
        else
            overheadLength={params.input_read_length}
            echo "User input sequence read length is: ${{overheadLength}}"
        fi 

        mkdir -p {output.star_out_dir}

        STAR --runMode alignReads \
             --runThreadN {params.n_threads} \
             --genomeDir {input.star_genome_index_dir} \
             --readFilesCommand zcat \
             --sjdbGTFfile {input.annotFile} \
             --readFilesIn {input.forwardTrimmed} {input.reverseTrimmed} \
             --outFileNamePrefix {params.star_sample_prefix} \
             --outSAMtype BAM SortedByCoordinate \
             --quantMode TranscriptomeSAM GeneCounts \
             --twopassMode Basic \
             --sjdbOverhang ${{overheadLength}} {params.user_added_starParams}
        """


## rule for picard collect rna seq metrics!
## this works
## picard.jar file needs to be local, NOT in the container, probs will download it in the move_reference_files rule or have it prewrapped in the pipeline
rule run_picard_collect_rna_seq:
    input:
        aligned_coordBam = "bulk_RNAseq_out/star_alignment/{sample}/{sample}.Aligned.sortedByCoord.out.bam",
        refFlat_file = NEW_PICARD_REFFLAT,
        riboIntList_file = NEW_PICARD_RRNA_INTERVAL
    output:
        collect_rnaSeq_file = "bulk_RNAseq_out/picard/{sample}/{sample}.picard.metrics.txt"
    singularity:
        PICARD_SING
    params:
        sample_out_dir = "bulk_RNAseq_out/picard/{sample}/",
        jar_file = "/opt/picard-2.27.5/picard.jar", ## tell it to look in the container for this, change permissions of jar file in container (exec java -jar picard.jar in the container)
        strandedness = "NONE"
    shell:
        """
        mkdir -p {params.sample_out_dir}

        java -jar {params.jar_file} CollectRnaSeqMetrics \
             I={input.aligned_coordBam} \
             O={output.collect_rnaSeq_file} \
             REF_FLAT={input.refFlat_file} \
             STRAND={params.strandedness} \
             RIBOSOMAL_INTERVALS={input.riboIntList_file}
        """



## rule for picard collect insert size metrics !
## this works
rule run_picard_collect_insert_size:
    input:
        aligned_coordBam = "bulk_RNAseq_out/star_alignment/{sample}/{sample}.Aligned.sortedByCoord.out.bam"
    output:
        insertSize_file = "bulk_RNAseq_out/picard/{sample}/{sample}.picard.insertSize.txt",
        insertSize_histogram = "bulk_RNAseq_out/picard/{sample}/{sample}.picard.insertSize_histogram.pdf"
    singularity:
        PICARD_SING
    params:
        sample_out_dir = "bulk_RNAseq_out/picard/{sample}/",
        jar_file = "/opt/picard-2.27.5/picard.jar" ## this is in the container!!
    shell:
        """
        # check if outDir exists; if it doesn't it will make the output directory
        ## idk if i need to do this or not 
        if [ -d {params.sample_out_dir} ];
        then
            echo "Directory {params.sample_out_dir} exists. Proceeding with analysis."
        else
            echo "Directory {params.sample_out_dir} does not exist. Generating directory..."
            mkdir -p {params.sample_out_dir}
        fi

        java -jar {params.jar_file} CollectInsertSizeMetrics \
             I={input.aligned_coordBam} \
             O={output.insertSize_file} \
             H={output.insertSize_histogram}
        """


## need a script here to calculate strandedness from picard collect rna seq metrics output BEFORE rsem!!
## write python function and put it in snake_functions.py to reference here 
## add automated strand interpretation column in picard collect rna seq metrics table (sanity checking)
## all picard for all samples should complete before we put them all into rule to wrangle picard results into same df and save as .csv
## use run: instead of shell: 
## do I need to import needed libraries in the run directive or will it be okay? - it wil be okay 
rule calculate_strandedness:
    input:
        collect_rnaSeq_files = expand("bulk_RNAseq_out/picard/{sample}/{sample}.picard.metrics.txt",
                                      sample=SAMPLE_LIST)
    output:
        allSample_picard_strandedness = "bulk_RNAseq_out/picard/allSample_picard_metrics.csv"
    params:
        picard_metrics_pattern = "bulk_RNAseq_out/picard/*/*.picard.metrics.txt"
    run:
        picard_calculate_strandedness(file_pattern=params.picard_metrics_pattern,
                                      out_file=output.allSample_picard_strandedness)



## rsem 
## need to include the genome name prefix for the rsem index files or else it loses its mind
## have to also provide the file prefix with the output directory path (I forgot this)
rule run_rsem_quantification:
    input:
        aligned_transcriptBam = "bulk_RNAseq_out/star_alignment/{sample}/{sample}.Aligned.toTranscriptome.out.bam",
        rsem_index_dir = "reference_indices/rsem/",
        picard_metrics_file = "bulk_RNAseq_out/picard/allSample_picard_metrics.csv"
    output:
        rsem_out_dir = directory("bulk_RNAseq_out/rsem_quantification/{sample}/")
    singularity:
        RSEM_SING
    params:
        nthreads = 6, ## nthreads = cpus_per_task*2 (for now)
        current_sample = lambda wc: wc.get("sample"),
        rsem_index_name = ALIGN_INDEX_NAME,
        user_added_rsemParams = EXTRA_RSEM_PARAMS
    shell:
        """
        ## use wildcard to pull the line for each sampleid and the value from the last column (rsem strandedness will always be in the last column)
        strandedness=$( grep "{params.current_sample}" {input.picard_metrics_file} | awk -F',' '{{print $NF}}' )

        ## grab second to last column value to print the strandedness for user understanding
        readableStrandedness=$( grep "{params.current_sample}" {input.picard_metrics_file} | awk -F',' '{{print $(NF-1)}}' )
        echo "Calculated strandedness of {params.current_sample} is: ${{readableStrandedness}}"

        mkdir -p {output.rsem_out_dir}

        rsem-calculate-expression --paired-end \
                                  --bam \
                                  -p {params.nthreads} {params.user_added_rsemParams} \
                                  --forward-prob ${{strandedness}} \
                                  --time {input.aligned_transcriptBam} {input.rsem_index_dir}/{params.rsem_index_name} {output.rsem_out_dir}/{params.current_sample}
        
        """