from os.path import join as pj
from os.path import basename as basefile
from os import environ
import pandas as pd
from snake_utils.snake_functions import comb_filepaths, make_fp_dict


RAW_SEQ_IN =  config['raw_seq_in']
METADATA = pd.read_csv(config['metadata_file'], sep='\t')
SAMPLE_LIST = METADATA['sampleid'].tolist()
MINI_SAMPLE_LIST = 'NA248plus_AUT_S9_L004'
ALIGN_FASTA = config['align_to_fasta']
ALIGN_ANNOT = config['align_to_gtf']
ALIGN_INDEX_NAME = config['align_to_name']
## to move wanted genome index files into the pipeline's working directory so bind points dont break 
REF_DIR = "test_refs"
NEW_FASTA_PATH= pj(REF_DIR, basefile(ALIGN_FASTA))
NEW_ANNOT_PATH = pj(REF_DIR, basefile(ALIGN_ANNOT))
READ_LENGTH = config['raw_seq_read_length']

## for rsem index generation 
RSEM_INDEX_DIR = pj("reference_indices/rsem", ALIGN_INDEX_NAME)


sample_fp_dict = make_fp_dict(metadata_df=METADATA,
                              dataset_dir=RAW_SEQ_IN)


## conda environment
FASTQC_CONDA = "envs/fastqc_env.yml"
MULTIQC_CONDA = "envs/multiqc_env.yml"

## singularities (preferred) - NEED TO DEBUG THIS ON ALPINE
FASTQC_SING = "docker://madiapgar/bulk_rna_seq:fastqc-v0.12.1"
MULTIQC_SING = "docker://madiapgar/bulk_rna_seq:multiqc-v1.26"
CUTADAPT_SING = "docker://madiapgar/bulk_rna_seq:cutadapt-v4.2"
STAR_SING = "docker://madiapgar/bulk_rna_seq:star-v2.7.10b"
RSEM_SING = "docker://madiapgar/bulk_rna_seq:rsem-v1.3.3"
PICARD_SING = "docker://madiapgar/bulk_rna_seq:picard-v2.27.5"

## functions? to use these as inputs you need to have the function defined in the snakefile which is dumb but whatever
def pull_rawSeq_fps(wildcards):
    raw_files = sample_fp_dict[wildcards.sample]
    return(raw_files)


# Set apptainer bindings NOT A PERMANENT SOLUTION
## idk if this is going to work
##os.environ["APPTAINER_BIND"] = "/scratch/alpine/mapgar@xsede.org:/scratch/alpine/mapgar@xsede.org"

rule all:
    input:
        expand("bulk_RNAseq_out/pretrimming_fastqc/{sample}/",
               sample=SAMPLE_LIST),
        "bulk_RNAseq_out/pretrimming_multiqc_report.html",
        expand("bulk_RNAseq_out/cutadapt/{sample}/{sample}_R1_trimmed.fastq.gz",
               sample=MINI_SAMPLE_LIST),
        expand("bulk_RNAseq_out/cutadapt/{sample}/{sample}_R2_trimmed.fastq.gz",
               sample=MINI_SAMPLE_LIST),
        expand("bulk_RNAseq_out/cutadapt/{sample}/{sample}_cutadapt.log",
               sample=MINI_SAMPLE_LIST),
        expand("bulk_RNAseq_out/cutadapt/{sample}/{sample}_cutadapt.err",
               sample=MINI_SAMPLE_LIST),
        expand("bulk_RNAseq_out/posttrimming_fastqc/{sample}/",
                sample=MINI_SAMPLE_LIST),
        "bulk_RNAseq_out/posttrimming_multiqc_report.html",
        NEW_FASTA_PATH,
        NEW_ANNOT_PATH,
        "reference_indices/rsem/",
        "reference_indices/star/",
        expand("bulk_RNAseq_out/star_alignment/{sample}/",
               sample=MINI_SAMPLE_LIST)
        

##### SUBWORKFLOW ONE!! #####
## set up to run for all paired samples - in .csv file that tells you what pairs on, read in as a dict of the file names per sample
## output directory per samples
## can keep threads=2 and ntasks=1
##### madis notes: #####
## both examples under "input:" work to pull both forward/reverse reads in for each sample 
## could also do this: inFiles = expand(pj(RAW_SEQ_IN, "{{sample}}_{read}.fastq.gz"), read=READS) 
rule run_pretrimming_fastqc:
    input:
        inFiles = pull_rawSeq_fps
    output:
        outDir = directory("bulk_RNAseq_out/pretrimming_fastqc/{sample}/")
    singularity:
        FASTQC_SING
    conda:
        FASTQC_CONDA
    params:
        threads = 2
    shell:
        """
        echo "running fastqc"

        mkdir {output.outDir}

        fastqc {input.inFiles} -o {output.outDir} --threads {params.threads}
        """



rule run_pretrimming_multiqc:
    input:
        ## tells snakemake to wait for all fastqc outputs to be made before starting this rule since all of them are required for this rule
        inDirs = expand("bulk_RNAseq_out/pretrimming_fastqc/{sample}/",
                         sample=SAMPLE_LIST)
    output:
        outFile = "bulk_RNAseq_out/pretrimming_multiqc_report.html"
    singularity:
        MULTIQC_SING
    conda:
        MULTIQC_CONDA
    params:
        outDir = "bulk_RNAseq_out/",
        multiqcFilename = "pretrimming_multiqc_report.html"
    shell:
        """
        echo "running multiqc"

        multiqc {input.inDirs} -o {params.outDir} --filename {params.multiqcFilename} .
        """

## {params.inDir}*/


## had to add set +o pipefail; to beginning of totalScores command so that snakemake doesn't detect non-zero error codes and fail
## (snakemake was detecing 141 error codes for first two steps in pipe bc pipe is so long they complete before later steps do)
## added '<' to zcat command so its portable to macos, macos prefers gzcat or zcat < but not plain zcat (its dumb)
## can use to check the error codes of all components of a pipe: echo ${PIPESTATUS[@]}
## snakemake can insert {params.whatever} inside the double quotes of chemistryFlag variable and its evaluated as the value (30)
rule run_cutadapt:
    input:
        inFiles = pull_rawSeq_fps
    output:
        forwardTrimmed = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_R1_trimmed.fastq.gz",
        reverseTrimmed = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_R2_trimmed.fastq.gz"
    singularity:
        CUTADAPT_SING
    ##conda:
    log:
        log = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_cutadapt.log",
        error = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_cutadapt.err"
    params:
        adapter_threePrime = "AGATCGGAAGAG",
        qualTrim = 30,
        minReadLen = 10,
        n_threads = 4 ## n_threads = cpus_per_task*2
    shell:
        """
        totalScores=$( set +o pipefail; zcat < {input.inFiles[0]} | awk 'NR%4==0' | sed 100q | grep -o . | sort | uniq | wc -l )

        if [ ${{totalScores}} -eq 4 ];
        then
            # NovaSeq now bins qc values to 2, 12, 23, 37 (if using NovaSeq), also use --nextseq-trim= instead of -q since NovaSeq is 2 color chemistry
            echo "Quality scores are binned to 4 values (your score = ${{totalScores}}) therefore, using 2-color chemistry; running --nextseq-trim instead of -q"
            chemistryFlag="--nextseq-trim={params.qualTrim}"
        else
            echo "Quality scores are not binned (your score = ${{totalScores}}), therefore use -q"
            chemistryFlag="-q {params.qualTrim}"
        fi

        cutadapt -a {params.adapter_threePrime} \
                 -A {params.adapter_threePrime} \
                 "${{chemistryFlag}}" \
                 --minimum-length={params.minReadLen} \
                 -j {params.n_threads} \
                 --pair-filter=any \
                 -o {output.forwardTrimmed} \
                 -p {output.reverseTrimmed} \
                 {input.inFiles[0]} {input.inFiles[1]}>{log.log} \
                 2>{log.error} 
        """

## can reuse previous rules this way!
## can alter the input, output, params, etc directives, anything not defined here will be inherited from the original rule
## CANNOT alter the execution step (i.e. shell)
## running fastqc on samples posttrimming via cutadapt
use rule run_pretrimming_fastqc as run_posttrimming_fastqc with:
    input:
        inFiles = ["bulk_RNAseq_out/cutadapt/{sample}/{sample}_R1_trimmed.fastq.gz", "bulk_RNAseq_out/cutadapt/{sample}/{sample}_R2_trimmed.fastq.gz"]
    output:
        outDir = directory("bulk_RNAseq_out/posttrimming_fastqc/{sample}/")


## running multiqc on samples posttrimming via cutadapt 
## if you include a directive to overwrite here (i.e. params), you'll need to specify all arguments underneath it,
## even if they don't change
use rule run_pretrimming_multiqc as run_posttrimming_multiqc with:
    input:
        inDirs = expand("bulk_RNAseq_out/posttrimming_fastqc/{sample}/",
                        sample=MINI_SAMPLE_LIST)
    output:
        outFile = "bulk_RNAseq_out/posttrimming_multiqc_report.html"
    params:
        outDir = "bulk_RNAseq_out/",
        multiqcFilename = "posttrimming_multiqc_report.html"


## move input fasta/gtf files into the test_snake_w_slurm (or whatever working) directory so bind points dont break
rule move_genome_index:
    input:
        fastaFile = ALIGN_FASTA,
        annotFile = ALIGN_ANNOT
    output:
        new_loc_fasta = NEW_FASTA_PATH,
        new_loc_annot = NEW_ANNOT_PATH
    params:
        ref_directory = REF_DIR
    shell:
        """
        mkdir -p {params.ref_directory}

        mv {input.fastaFile} {output.new_loc_fasta}
        echo "moved genome fasta file to working directory!"

        mv {input.annotFile} {output.new_loc_annot}
        echo "moved genome annotation file to working directory!"
        """

## wont want to run this rule unless its needed - config option/looking at star log files for %uniquely_mapped v %percent_multimapped v %unmapped thresholds for when its run
## rsem genome index generation (probs will be moved to a subworkflow but its here for now) 
## finally works using outside fasta/gtf files with above rule (yay!!)
rule generate_rsem_index:
    input:
        fastaFile = NEW_FASTA_PATH,
        annotFile = NEW_ANNOT_PATH
    output:
        rsem_index = directory("reference_indices/rsem/")
    singularity:
        RSEM_SING
    params:
        decoy_directory = "reference_indices/rsem/",
        rsem_index_name = ALIGN_INDEX_NAME
    shell:
        """
        mkdir -p {params.decoy_directory}

        rsem-prepare-reference --gtf {input.annotFile} {input.fastaFile} {params.decoy_directory}{params.rsem_index_name}
        """

## got errors about bc not being installed in container (it wasnt so thats fine) and out of memory error which was weird 
## increased memory to check if it does okay - needed 50GBs of memory 
## check if read length set to zero or do want to infer? can take user input parameter (greater than 0) - i cant remember why we were doing this
rule generate_star_index:
    input:
        pretrimming_multiqc_report = "bulk_RNAseq_out/pretrimming_multiqc_report_data/multiqc_general_stats.txt",
        fastaFile = NEW_FASTA_PATH,
        annotFile = NEW_ANNOT_PATH
    output:
        star_index = directory("reference_indices/star/")
    singularity:
        STAR_SING
    params:
        input_read_length = READ_LENGTH,
        n_threads = 22, ## n_threads = cpus_per_task*2
        decoy_directory = "reference_indices/star/",
        feature_type = "exon"
    shell:
        """
        if [ {params.input_read_length} -eq 0 ];
        then
            ## count read lengths in raw seq files for STAR
            readLength=$( awk '{{print $5}}' {input.pretrimming_multiqc_report} | tail -n+2 | head -n 1 )
            overheadLength=$( echo "$((${{readLength}}-1))" )
            echo "Inferred sequence read length is: ${{overheadLength}}"
        else
            overheadLength={params.input_read_length}
            echo "User input sequence read length is: ${{overheadLength}}"
        fi

        ## calculation of genomeLength - remove headers, remove whitespace and new lines, then count number of characters (bases)
        genomeLength=$(cat {input.fastaFile} | grep -v "^>" | tr -d [:space:] | wc -m)
        echo "The length of the provided genome is: ${{genomeLength}} bases"

        ## calculation of genomeSAindex: min(14, log2(GenomeLength)/2 - 1)
        genome_indexSize=$(printf %.0f $(echo "((l(${{genomeLength}})/l(2))/2)-1" | bc -l))
        if [[ ${{genome_indexSize}} -ge 14 ]]; then
            genome_indexSize=14
        fi
        echo "Using genome index size of: ${{genome_indexSize}}"

        ## making output directory
        mkdir -p {params.decoy_directory}

        STAR --runThreadN {params.n_threads} \
             --runMode genomeGenerate \
             --genomeDir {params.decoy_directory} \
             --genomeFastaFiles {input.fastaFile} \
             --sjdbGTFfile {input.annotFile} \
             --sjdbGTFfeatureExon {params.feature_type} \
             --sjdbOverhang ${{overheadLength}} \
             --genomeSAindexNbases ${{genome_indexSize}}

        """

## actual rule for star!!
## need to actually run this and debug!
rule run_star_alignment:
    input:
        pretrimming_multiqc_report = "bulk_RNAseq_out/pretrimming_multiqc_report_data/multiqc_general_stats.txt",
        star_genome_index_dir = "reference_indices/star/",
        forwardTrimmed = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_R1_trimmed.fastq.gz",
        reverseTrimmed = "bulk_RNAseq_out/cutadapt/{sample}/{sample}_R2_trimmed.fastq.gz",
        annotFile = NEW_ANNOT_PATH
    output:
        star_out_dir = directory("bulk_RNAseq_out/star_alignment/{sample}/")
    singularity:
        STAR_SING
    params:
        input_read_length = READ_LENGTH,
        n_threads = 28, ## n_threads = cpus_per_task*2
        star_sample_prefix = "bulk_RNAseq_out/star_alignment/{sample}/{sample}." ## may need another decoy here bc snakemake takes the front slash off of the output fp above (now the file names look funky)
    shell:
        """
        ## have to redo read length calculation, maybe not the best? but idk what else to do 
        if [ {params.input_read_length} -eq 0 ];
        then
            ## count read lengths in raw seq files for STAR
            readLength=$( awk '{{print $5}}' {input.pretrimming_multiqc_report} | tail -n+2 | head -n 1 )
            overheadLength=$( echo "$((${{readLength}}-1))" )
            echo "Inferred sequence read length is: ${{overheadLength}}"
        else
            overheadLength={params.input_read_length}
            echo "User input sequence read length is: ${{overheadLength}}"
        fi 

        mkdir -p {output.star_out_dir}

        STAR --runMode alignReads \
             --runThreadN {params.n_threads} \
             --genomeDir {input.star_genome_index_dir} \
             --readFilesCommand zcat \
             --sjdbGTFfile {input.annotFile} \
             --readFilesIn {input.forwardTrimmed} {input.reverseTrimmed} \
             --outFileNamePrefix {params.star_sample_prefix} \
             --outSAMtype BAM SortedByCoordinate \
             --quantMode TranscriptomeSAM GeneCounts \
             --twopassMode Basic \
             --sjdbOverhang ${{overheadLength}}
        """




