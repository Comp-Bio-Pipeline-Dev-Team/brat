## workflow profile
## specifies resources for certain rules (workflow specific)
## set via --workflow-profile flag in snakemake command
executor: slurm

default-resources:
    slurm_partition: "amilan"
    slurm_account: "amc-general"


## to define resources for a specific rule:
## NOTE: using cpus_per_task: instead of ntasks: to request number of cores bc we're calling snakemake from within a slurm job already 
## so it automatically downscales the requested cores to 1 (this is annoying)
## in alpine, 1 core = 3.75GB memory (divide memory request by 3.75 to get the correct number of cores to request)
## currently have threading to be 2*number of cores (hardcoded in snakefile)
set-resources:
    run_pretrimming_fastqc:
        runtime: "1h"
        mem: "1GB"
        cpus_per_task: 1
        nodes: 1
        slurm_extra: "'--qos=normal' '--output=logs/06202025/pretrimming_fastqc_%J.log' '--error=logs/06202025/pretrimming_fastqc_%J.err' '--mail-type=FAIL,END' '--mail-user=MADISON.APGAR@CUANSCHUTZ.EDU'"
    run_pretrimming_multiqc:
        runtime: "1h"
        mem: "1GB"
        cpus_per_task: 1
        nodes: 1
        slurm_extra: "'--qos=normal' '--output=logs/06202025/pretrimming_multiqc_%J.log' '--error=logs/06202025/pretrimming_multiqc_%J.err' '--mail-type=FAIL,END' '--mail-user=MADISON.APGAR@CUANSCHUTZ.EDU'" 
    run_cutadapt:
        runtime: "2h"
        mem: "5GB"
        cpus_per_task: 2
        nodes: 1
        slurm_extra: "'--qos=normal' '--output=logs/06202025/cutadapt_%J.log' '--error=logs/06202025/cutadapt_%J.err' '--mail-type=FAIL,END' '--mail-user=MADISON.APGAR@CUANSCHUTZ.EDU'"
    run_posttrimming_fastqc:
        runtime: "1h"
        mem: "1GB"
        cpus_per_task: 1
        nodes: 1
        slurm_extra: "'--qos=normal' '--output=logs/06202025/posttrimming_fastqc_%J.log' '--error=logs/06202025/posttrimming_fastqc_%J.err' '--mail-type=FAIL,END' '--mail-user=MADISON.APGAR@CUANSCHUTZ.EDU'"
    run_posttrimming_multiqc:
        runtime: "1h"
        mem: "1GB"
        cpus_per_task: 1
        nodes: 1
        slurm_extra: "'--qos=normal' '--output=logs/06202025/posttrimming_multiqc_%J.log' '--error=logs/06202025/posttrimming_multiqc_%J.err' '--mail-type=FAIL,END' '--mail-user=MADISON.APGAR@CUANSCHUTZ.EDU'"
    move_genome_index:
        runtime: "1h"
        mem: "1GB"
        cpus_per_task: 1
        nodes: 1
        slurm_extra: "'--qos=normal' '--output=logs/06202025/move_genome_index_%J.log' '--error=logs/06202025/move_genome_index_%J.err' '--mail-type=FAIL,END' '--mail-user=MADISON.APGAR@CUANSCHUTZ.EDU'" 
    generate_rsem_index:
        runtime: "1h"
        mem: "5GB"
        cpus_per_task: 1
        nodes: 1
        slurm_extra: "'--qos=normal' '--output=logs/06202025/outside_rsem_index_%J.log' '--error=logs/06202025/outside_rsem_index_%J.err' '--mail-type=FAIL,END' '--mail-user=MADISON.APGAR@CUANSCHUTZ.EDU'"
    generate_star_index:
        runtime: "1h"
        mem: "45GB"
        cpus_per_task: 11 ## 45/3.75 = cpus_per_task
        nodes: 1
        slurm_extra: "'--qos=normal' '--output=logs/06202025/star_index_%J.log' '--error=logs/06202025/star_index_%J.err' '--mail-type=FAIL,END' '--mail-user=MADISON.APGAR@CUANSCHUTZ.EDU'"
    run_star_alignment:
        runtime: "2h"
        mem: "50GB"
        cpus_per_task: 14 ## 50/3.75 = cpus_per_task
        nodes: 1
        slurm_extra: "'--qos=normal' '--output=logs/06202025/star_alignment_%J.log' '--error=logs/06202025/star_alignment_%J.err' '--mail-type=FAIL,END' '--mail-user=MADISON.APGAR@CUANSCHUTZ.EDU'"